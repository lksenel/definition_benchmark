{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/mounts/work/kerem/definition_classification_data/word_classificiations_from_definitions/mlm_with_log_probs/gpt2_n_results.pickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-56c6fac5bcb6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mall_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m                 \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults_data_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'{model_name}_{word_type}_results.pickle'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m                     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/mounts/work/kerem/definition_classification_data/word_classificiations_from_definitions/mlm_with_log_probs/gpt2_n_results.pickle'"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "from definition_processor import get_patterns\n",
    "from evaluate import SampleResult\n",
    "import numpy as np\n",
    "import pickle\n",
    "import nltk\n",
    "import os\n",
    "\n",
    "model_names = {\n",
    "    'mlm': ['bert-base-uncased', 'bert-large-uncased', 'roberta-base', 'roberta-large'],\n",
    "    'lm': ['gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl']\n",
    "}\n",
    "word_types = {\n",
    "    'n': 'Nouns',\n",
    "    'v': 'Verbs'\n",
    "}\n",
    "\n",
    "word_counts_file = '/mounts/work/kerem/datasets/WordNet/wordnet_word_counts_in_WWC.pickle'\n",
    "with open(word_counts_file, 'rb') as handle:\n",
    "    word_counts = pickle.load(handle)\n",
    "    \n",
    "for model_type in ['mlm']:#,'lm']:\n",
    "    for word_type in ['n','v']:\n",
    "        for detail_level in ['tokenization','pattern','frequency']:\n",
    "\n",
    "            max_token_count = 3 # for tokenization_level results\n",
    "            count_thresholds = [10, 100] # for frequency level results, should contain only 2 numbers\n",
    "            pattern_count = len(get_patterns(model_type, word_type))\n",
    "\n",
    "#             results_data_dir = '/mounts/work/kerem/definition_classification_data/results'\n",
    "#             results_print_dir = '/mounts/work/kerem/gitlab_projects/bertram_with_informative_contexts/evaluate/definition_classification/results'\n",
    "\n",
    "            results_data_dir = '/mounts/work/kerem/definition_classification_data/word_classificiations_from_definitions/mlm_with_log_probs'\n",
    "            results_print_dir = '/mounts/work/kerem/gitlab_projects/bertram_with_informative_contexts/evaluate/word_clasification_from_definitions/results'\n",
    "\n",
    "    \n",
    "            results_file = os.path.join(results_print_dir, f'{model_type}_log_prob_{detail_level}_results_for_WordNet_{word_types[word_type]}.txt')\n",
    "\n",
    "\n",
    "            headers = ['Model']\n",
    "            if detail_level == 'pattern':\n",
    "                for no in range(pattern_count):\n",
    "                    headers.append(f'Pattern {no+1}')\n",
    "                headers.append('Best Pattern')\n",
    "\n",
    "            elif detail_level == 'frequency':\n",
    "                headers.append(f'rare (0-{count_thresholds[0]-1})')\n",
    "                headers.append(f'medium ({count_thresholds[0]}-{count_thresholds[1]-1})')\n",
    "                headers.append(f'freq. ({count_thresholds[1]}-)')\n",
    "                headers.append(f'all')\n",
    "            elif detail_level == 'tokenization':\n",
    "                for i in range(max_token_count):\n",
    "                    headers.append(f'{i+1} token')\n",
    "                headers.append(f'{max_token_count}+ token')\n",
    "                headers.append(f'all')\n",
    "            else:\n",
    "                print('Invalid detail_level!')\n",
    "\n",
    "            models = model_names[model_type].copy()\n",
    "            all_results = {}\n",
    "            for model_name in models:\n",
    "                with open(os.path.join(results_data_dir, f'{model_name}_{word_type}_results.pickle'), 'rb') as handle:\n",
    "                    results = pickle.load(handle)\n",
    "\n",
    "                accuracies = []\n",
    "                pred_scores = []\n",
    "                option_counts = []\n",
    "                for i in range(len(headers)-1):\n",
    "                    pred_scores.append([])\n",
    "                    accuracies.append([])\n",
    "                    option_counts.append([])\n",
    "\n",
    "                no = 0\n",
    "                for key, value in results.items():\n",
    "                    word = key.split('.')[0].replace(\"_\", \" \")\n",
    "                    tokens = nltk.word_tokenize(word)\n",
    "                    word = ' '.join(token.lower() for token in tokens)\n",
    "#                     token_count = len(value[0].tokenized_word)\n",
    "                    token_count = len(value[0].tokenized_words[value[0].answer])\n",
    "                        \n",
    "                    scores = []\n",
    "                    correct = 0\n",
    "                    for pattern_no in range(pattern_count):\n",
    "                        score = value[pattern_no].prediction_score\n",
    "                        scores.append(score)\n",
    "\n",
    "                        if score == 1:                   \n",
    "                            correct = 1\n",
    "\n",
    "                        if detail_level == 'pattern':\n",
    "                            accuracies[pattern_no].append(int(score == 1))\n",
    "                            pred_scores[pattern_no].append(score)\n",
    "\n",
    "                    if 'random' in model_name:\n",
    "                        best_pred_score = score\n",
    "                        correct = 1 if score == 1 else 0\n",
    "                    else:\n",
    "                        best_pred_score = max(scores)\n",
    "\n",
    "                    if detail_level == 'frequency':\n",
    "                        freq_level = int(word_counts[word] >= count_thresholds[0]) + int(word_counts[word] >= count_thresholds[1])\n",
    "                        accuracies[freq_level].append(correct)\n",
    "                        pred_scores[freq_level].append(best_pred_score)\n",
    "                        option_counts[freq_level].append(len(value[0].prediction_probs))\n",
    "                    \n",
    "                    if detail_level == 'tokenization':\n",
    "                        pos = min(token_count, max_token_count+1)-1\n",
    "                        accuracies[pos].append(correct)\n",
    "                        pred_scores[pos].append(best_pred_score)\n",
    "                        option_counts[pos].append(len(value[0].prediction_probs))\n",
    "                        \n",
    "                    accuracies[-1].append(correct)\n",
    "                    pred_scores[-1].append(best_pred_score)\n",
    "                    option_counts[-1].append(len(value[0].prediction_probs))\n",
    "                                    \n",
    "                mean_pred_scores = [np.round(np.mean(scores), 2) for scores in pred_scores]\n",
    "                mean_accuracies = [np.round(np.mean(acc)*100,2) for acc in accuracies]\n",
    "                all_results[model_name] = (mean_pred_scores, mean_accuracies)\n",
    "                \n",
    "                \n",
    "                sample_counts = []\n",
    "                mean_option_counts = []\n",
    "                for i in range(len(headers)-1):\n",
    "                    sample_counts.append(len(accuracies[i]))\n",
    "                    mean_option_counts.append(np.round(np.mean(option_counts[i]),1))       \n",
    "\n",
    "            \n",
    "            if detail_level == 'frequency':\n",
    "                mean_pred_scores = []\n",
    "                mean_accuracies = []\n",
    "                for i in range(len(headers)-1):   \n",
    "                    mean_pred_scores.append(0.5)\n",
    "                    mean_accuracies.append(sum([1/count for count in option_counts[i]])/len(option_counts[i])*100)\n",
    "                all_results['random'] = (mean_pred_scores, mean_accuracies)\n",
    "\n",
    "            with open(results_file, 'w') as f_out:\n",
    "                f_out.write(f'Results from {model_type} for Wordnet {word_types[word_type]}')\n",
    "                f_out.write(f'\\n\\nExample count: {len(option_counts[-1])}')\n",
    "                f_out.write(f'\\nAverage option count: {np.mean(option_counts[-1]):.2f}')\n",
    "                f_out.write(f'\\nMinimum option count: {np.min(option_counts[-1])}')\n",
    "                f_out.write(f'\\nMaximum option count: {np.max(option_counts[-1])}')\n",
    "\n",
    "                if detail_level == 'frequency' or detail_level == 'tokenization':\n",
    "                    for no, header in enumerate(headers[1:-1]):\n",
    "                        f_out.write(f'\\n\\nFor {header}:')\n",
    "                        f_out.write(f'\\nExample count: {len(option_counts[no])}')\n",
    "                        f_out.write(f'\\nAverage option count: {mean_option_counts[no]:.2f}')\n",
    "\n",
    "\n",
    "                f_out.write('\\n\\n\\nPrediction Rank Results\\n\\n')\n",
    "                model_no = 0\n",
    "                tab = []\n",
    "                for model, results in all_results.items():\n",
    "                    tab.append([])\n",
    "                    tab[model_no].append(model)\n",
    "                    tab[model_no].extend(results[0])\n",
    "                    model_no += 1\n",
    "                f_out.write(tabulate(tab, headers=headers, tablefmt='orgtbl'))\n",
    "\n",
    "\n",
    "                f_out.write('\\n\\n\\nAccuracy Results\\n\\n')\n",
    "                model_no = 0\n",
    "                tab = []\n",
    "                for model, results in all_results.items():\n",
    "                    tab.append([])\n",
    "                    tab[model_no].append(model)\n",
    "                    tab[model_no].extend(results[1])\n",
    "                    model_no += 1\n",
    "                f_out.write(tabulate(tab, headers=headers, tablefmt='orgtbl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Investigate results for rare words\n",
    "from tabulate import tabulate\n",
    "\n",
    "from evaluate import SampleResult\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "results_dir = '/mounts/work/kerem/definition_classification_data/results'\n",
    "\n",
    "model_names = [\n",
    "    'bert-base-uncased',#'embeddings-wiki-100-diff-bbu-all_bert-base-uncased',\n",
    "    'bertram_original_context_bert-base-uncased','bertram_original_add_bert-base-uncased',\n",
    "    'bertram_CE_context_bert-base-uncased', 'bertram_CE_add_bert-base-uncased',\n",
    "    'bertram_CE_old_context_bert-base-uncased', 'bertram_CE_old_fused_bert-base-uncased',\n",
    "#     'bertram_CE_old_model_add_bert-base-uncased', 'bertram_random_informativeness_context_bert-base-uncased',\n",
    "#     'bertram_uniform_informativeness_context_bert-base-uncased',\n",
    "#     'ablation_experiment'\n",
    "              ]\n",
    "\n",
    "short_model_names = {\n",
    "    'bert-base-uncased': 'bert',\n",
    "    'embeddings-wiki-100-diff-bbu-all_bert-base-uncased': 'OTA',\n",
    "    'bertram_original_context_bert-base-uncased': 'AM context',\n",
    "    'bertram_original_add_bert-base-uncased': 'AM add',\n",
    "    'bertram_CE_context_bert-base-uncased': 'CE context',\n",
    "    'bertram_CE_add_bert-base-uncased': 'CE add',\n",
    "    'bertram_CE_old_context_bert-base-uncased': 'CE old context',\n",
    "    'bertram_CE_old_fused_bert-base-uncased': 'CE old fused',\n",
    "    'bertram_CE_old_add_bert-base-uncased': 'CE old add',\n",
    "    'bertram_random_informativeness_context_bert-base-uncased': 'random informativeness',\n",
    "    'bertram_uniform_informativeness_context_bert-base-uncased': 'uniform informativeness',\n",
    "    'ablation_experiment': 'random'\n",
    "}\n",
    "\n",
    "\n",
    "word_counts_file = '/mounts/work/kerem/datasets/WordNet/wordnet_word_counts_in_WWC.pickle'\n",
    "with open(word_counts_file, 'rb') as handle:\n",
    "    word_counts = pickle.load(handle)\n",
    "    \n",
    "min_count = 1e10\n",
    "min_model = ''\n",
    "for model_name in model_names:\n",
    "    with open(os.path.join(results_dir, f'{model_name}_n_results.pickle'), 'rb') as handle:\n",
    "        results = pickle.load(handle)\n",
    "#     print(f'Model: {model_name}, result count: {len(results)}')\n",
    "    if len(results) < min_count:\n",
    "        min_count = len(results)\n",
    "        min_model = model_name\n",
    "\n",
    "with open(os.path.join(results_dir, f'{min_model}_n_results.pickle'), 'rb') as handle:\n",
    "    results = pickle.load(handle)\n",
    "min_vocab = list(results.keys())\n",
    "\n",
    "\n",
    "for detail_level in ['tokenization','pattern','frequency']:\n",
    "\n",
    "    max_token_count = 3 # for tokenization_level results\n",
    "    count_thresholds = [10, 100] # for frequency level results, should contain only 2 numbers\n",
    "    pattern_count = len(get_patterns('mlm', 'n'))\n",
    "\n",
    "    results_data_dir = '/mounts/work/kerem/definition_classification_data/results'\n",
    "    results_print_dir = '/mounts/work/kerem/gitlab_projects/bertram_with_informative_contexts/evaluate/definition_classification/results'\n",
    "\n",
    "    results_file = os.path.join(results_print_dir, f'rare_words_{detail_level}_results_for_WordNet_Nouns.txt')\n",
    "\n",
    "    headers = ['Model']\n",
    "    if detail_level == 'pattern':\n",
    "        for no in range(pattern_count):\n",
    "            headers.append(f'Pattern {no+1}')\n",
    "        headers.append('Best Pattern')\n",
    "\n",
    "    elif detail_level == 'frequency':\n",
    "        headers.append(f'rare (0-{count_thresholds[0]-1})')\n",
    "        headers.append(f'medium ({count_thresholds[0]}-{count_thresholds[1]-1})')\n",
    "        headers.append(f'freq. ({count_thresholds[1]}-)')\n",
    "        headers.append(f'all')\n",
    "    elif detail_level == 'tokenization':\n",
    "        for i in range(max_token_count):\n",
    "            headers.append(f'{i+1} token')\n",
    "        headers.append(f'{max_token_count}+ token')\n",
    "        headers.append(f'all')\n",
    "    else:\n",
    "        print('Invalid detail_level!')\n",
    "\n",
    "    models = model_names.copy()\n",
    "    if detail_level != 'tokenization':\n",
    "        models.append(f'random_with_{pattern_count}_patterns')              \n",
    "                \n",
    "    all_results = {}\n",
    "    for model_name in models:\n",
    "        with open(os.path.join(results_data_dir, f'{model_name}_n_results.pickle'), 'rb') as handle:\n",
    "            results = pickle.load(handle)\n",
    "\n",
    "        if model_name == 'bert-base-uncased':\n",
    "            tokenization_lens = {}\n",
    "            for key, value in results.items():\n",
    "                if key in min_vocab:\n",
    "                    tokenization_lens[key] = len(value[0].tokenized_word)\n",
    "            \n",
    "        accuracies = []\n",
    "        pred_scores = []\n",
    "        option_counts = []\n",
    "        for i in range(len(headers)-1):\n",
    "            pred_scores.append([])\n",
    "            accuracies.append([])\n",
    "            option_counts.append([])\n",
    "\n",
    "        no = 0\n",
    "        for key, value in results.items():\n",
    "            if key in min_vocab:\n",
    "                word = key.split('.')[0].replace(\"_\", \" \")\n",
    "                tokens = nltk.word_tokenize(word)\n",
    "                word = ' '.join(token.lower() for token in tokens)\n",
    "                token_count = tokenization_lens[key]\n",
    "\n",
    "                scores = []\n",
    "                correct = 0\n",
    "                for pattern_no in range(pattern_count):\n",
    "                    score = value[pattern_no].prediction_score\n",
    "                    scores.append(score)\n",
    "\n",
    "                    if score == 1:                   \n",
    "                        correct = 1\n",
    "\n",
    "                    if detail_level == 'pattern':\n",
    "                        accuracies[pattern_no].append(int(score == 1))\n",
    "                        pred_scores[pattern_no].append(score)\n",
    "\n",
    "                if 'random' in model_name:\n",
    "                    best_pred_score = score\n",
    "                    correct = 1 if score == 1 else 0\n",
    "                else:\n",
    "                    best_pred_score = max(scores)\n",
    "\n",
    "                if detail_level == 'frequency':\n",
    "                    freq_level = int(word_counts[word] >= count_thresholds[0]) + int(word_counts[word] >= count_thresholds[1])\n",
    "                    accuracies[freq_level].append(correct)\n",
    "                    pred_scores[freq_level].append(best_pred_score)\n",
    "                    option_counts[freq_level].append(len(value[0].prediction_probs))\n",
    "\n",
    "                if detail_level == 'tokenization':\n",
    "                    pos = min(token_count, max_token_count+1)-1\n",
    "                    accuracies[pos].append(correct)\n",
    "                    pred_scores[pos].append(best_pred_score)\n",
    "                    option_counts[pos].append(len(value[0].prediction_probs))\n",
    "\n",
    "                accuracies[-1].append(correct)\n",
    "                pred_scores[-1].append(best_pred_score)\n",
    "                option_counts[-1].append(len(value[0].prediction_probs))\n",
    "\n",
    "        mean_pred_scores = [np.round(np.mean(scores), 2) for scores in pred_scores]\n",
    "        mean_accuracies = [np.round(np.mean(acc)*100,2) for acc in accuracies]\n",
    "        try:\n",
    "            all_results[short_model_names[model_name]] = (mean_pred_scores, mean_accuracies)\n",
    "        except:\n",
    "            all_results[model_name] = (mean_pred_scores, mean_accuracies)\n",
    "\n",
    "        sample_counts = []\n",
    "        mean_option_counts = []\n",
    "        for i in range(len(headers)-1):\n",
    "            sample_counts.append(len(accuracies[i]))\n",
    "            mean_option_counts.append(np.round(np.mean(option_counts[i]),1))       \n",
    "\n",
    "    with open(results_file, 'w') as f_out:\n",
    "        f_out.write(f'Results from bert-base-uncase for Wordnet {word_types[word_type]}')\n",
    "        f_out.write(f'\\n\\nExample count: {len(option_counts[-1])}')\n",
    "        f_out.write(f'\\nAverage option count: {np.mean(option_counts[-1]):.2f}')\n",
    "        f_out.write(f'\\nMinimum option count: {np.min(option_counts[-1])}')\n",
    "        f_out.write(f'\\nMaximum option count: {np.max(option_counts[-1])}')\n",
    "\n",
    "        if detail_level == 'frequency' or detail_level == 'tokenization':\n",
    "            for no, header in enumerate(headers[1:-1]):\n",
    "                f_out.write(f'\\n\\nFor {header}:')\n",
    "                f_out.write(f'\\nExample count: {len(option_counts[no])}')\n",
    "                f_out.write(f'\\nAverage option count: {mean_option_counts[no]:.2f}')\n",
    "\n",
    "\n",
    "        f_out.write('\\n\\n\\nPrediction Rank Results\\n\\n')\n",
    "        model_no = 0\n",
    "        tab = []\n",
    "        for model, results in all_results.items():\n",
    "            tab.append([])\n",
    "            tab[model_no].append(model)\n",
    "            tab[model_no].extend(results[0])\n",
    "            model_no += 1\n",
    "        f_out.write(tabulate(tab, headers=headers, tablefmt='orgtbl'))\n",
    "\n",
    "\n",
    "        f_out.write('\\n\\n\\nAccuracy Results\\n\\n')\n",
    "        model_no = 0\n",
    "        tab = []\n",
    "        for model, results in all_results.items():\n",
    "            tab.append([])\n",
    "            tab[model_no].append(model)\n",
    "            tab[model_no].extend(results[1])\n",
    "            model_no += 1\n",
    "        f_out.write(tabulate(tab, headers=headers, tablefmt='orgtbl'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.18232156, 1.00000668, 1.16315081, 1.43508453])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.asarray([1.2,2.7183,3.2,4.2])\n",
    "b = np.log(a)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
