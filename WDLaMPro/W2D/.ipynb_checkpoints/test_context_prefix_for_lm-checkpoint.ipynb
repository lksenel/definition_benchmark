{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "from random\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "DEFINITION_TOKEN = '<DEF>'\n",
    "\n",
    "def get_candidates(syn):\n",
    "    definition_set = set()\n",
    "    option_names = set()\n",
    "    options = {}\n",
    "\n",
    "    # Add the target first\n",
    "    target_name = syn.name().split('.')[0].replace(\"_\", \" \")\n",
    "    target_def = syn.definition()\n",
    "    options[syn.name()] = target_def\n",
    "    option_names.add(target_name)\n",
    "    definition_set.add(target_def)\n",
    "\n",
    "    for hypernym in syn.hypernyms():\n",
    "        for option in hypernym.hyponyms():\n",
    "            option_name = option.name().split('.')[0].replace(\"_\", \" \")\n",
    "            definition = option.definition()\n",
    "            if (option_name not in option_names) and (definition not in definition_set):\n",
    "                options[option.name()] = definition\n",
    "                option_names.add(option_name)\n",
    "                definition_set.add(definition)   \n",
    "\n",
    "    definitions = list(options.values())\n",
    "    option_words = list(options.keys())\n",
    "    \n",
    "    return option_words, definitions\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "model_name = 'gpt2-xl'\n",
    "syn_name = 'boondoggle.n.01' \n",
    "# syn_name = 'clanger.n.01'\n",
    "# syn_name = 'block_vote.n.01'\n",
    "gpu_id = 0\n",
    "\n",
    "batch_size = 1\n",
    "padded_context_counts = [0,1,3]\n",
    "\n",
    "pattern = f'{DEFINITION_TOKEN} is the definition of'\n",
    "\n",
    "\n",
    "target_word = syn_name.split('.')[0].replace(\"_\", \" \")\n",
    "\n",
    "\n",
    "with open(\"/mounts/work/kerem/datasets/WordNet/wordnet_words_max_count_100_contexts.txt\") as context_file:\n",
    "    for line in context_file:\n",
    "        word = line.split('\\t')[0]\n",
    "        if word == ' '.join(nltk.tokenize(target_word)):\n",
    "            target_word_contexts = line.split('\\t')[1:] \n",
    "            break\n",
    "\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "device = torch.device(f\"cuda:{gpu_id}\" if torch.cuda.is_available() else \"cpu\")   \n",
    "model.to(device)\n",
    "\n",
    "softmax = torch.nn.functional.softmax\n",
    "\n",
    "\n",
    "syn = wn.synset(syn_name)\n",
    "\n",
    "option_words, definitions = get_candidates(syn)\n",
    "answer = option_words.index(syn.name())\n",
    "option_count = len(option_words)\n",
    "\n",
    "target_ids = tokenizer.encode( \" \" + target_word, add_special_tokens=False)\n",
    "\n",
    "for padded_context_count in padded_context_counts:\n",
    "    contexts_to_pad = random.sample(target_word_contexts, padded_context_count)\n",
    "\n",
    "    # Process the input\n",
    "    start_inds = []\n",
    "    contexts = []\n",
    "    def_ind = pattern.split().index(DEFINITION_TOKEN)     \n",
    "    for option_def in definitions: \n",
    "        defin = option_def.capitalize()\n",
    "\n",
    "        context = ' '.join(contexts_to_pad.extend(pattern.replace(DEFINITION_TOKEN, defin)))\n",
    "        contexts.append(context)\n",
    "        start_inds.append(len(self.tokenizer.encode(context))-1)\n",
    "\n",
    "    inputs = self.tokenizer(contexts, padding=True)           \n",
    "    \n",
    "    seq_len = len(inputs['input_ids'][0])\n",
    "    \n",
    "    print(f'Prepadding {padded_context_count} contexts before definition')\n",
    "    print(f'Input sequence length is {seq_len} tokens')\n",
    "    \n",
    "    \n",
    "    input_ids = inputs['input_ids']\n",
    "    seq_mask = inputs['attention_mask']\n",
    "    \n",
    "    split_inds = np.array_split(list(range(option_count)), np.ceil(option_count/batch_size))\n",
    "    \n",
    "    prediction_probs = np.ones([option_count])\n",
    "    for gen_no, target_id in enumerate(target_ids):\n",
    "        sample_no = 0\n",
    "        for inds in split_inds:\n",
    "            input_ids_split = input_ids[inds].to(device)\n",
    "            seq_mask_split = seq_mask[inds].to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                output = self.model(input_ids=input_ids_split, attention_mask=seq_mask_split)[0]\n",
    "\n",
    "            for sample_no_split in range(len(input_ids_split)):\n",
    "                target_ind = sum(seq_mask_split[sample_no_split])-1\n",
    "                prob = softmax(output[sample_no_split, target_ind,:], dim=0)[target_id].cpu().numpy()    \n",
    "                prediction_probs[sample_no] *= prob\n",
    "                \n",
    "                sample_no += 1\n",
    "                \n",
    "                if target_ind == (seq_len - 1):\n",
    "                    input_ids[input_no] = torch.cat((input_ids[input_no, 1:], torch.tensor(target_id).unsqueeze(0).to(self.device)),0) \n",
    "                else:\n",
    "                    input_ids[input_no, target_location + 1] = target_id\n",
    "                    seq_mask[input_no, target_location + 1] = 1\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
